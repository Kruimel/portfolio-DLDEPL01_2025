{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13d18d4",
   "metadata": {},
   "source": [
    "Define settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47619088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import ReportTypes, Trainer, TrainerSettings, metrics\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "\n",
    "batch_size = 64\n",
    "# batch_size = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78a2f1",
   "metadata": {},
   "source": [
    "Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9492ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "from typing import Iterator\n",
    "\n",
    "def get_streamers(batchsize: int) -> tuple[Iterator, Iterator]:\n",
    "    fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "    preprocessor = BasePreprocessor()\n",
    "    streamers = fashionfactory.create_datastreamer(\n",
    "        batchsize=batchsize, preprocessor=preprocessor\n",
    "    )\n",
    "    train = streamers[\"train\"]\n",
    "    valid = streamers[\"valid\"]\n",
    "    trainstreamer = train.stream()\n",
    "    validstreamer = valid.stream()\n",
    "    output_size = 10\n",
    "    return trainstreamer, validstreamer, output_size\n",
    "\n",
    "# def get_streamers(batchsize: int) -> tuple[Iterator, Iterator]:\n",
    "#     flowersfactory = DatasetFactoryProvider.create_factory(DatasetType.FLOWERS)\n",
    "#     preprocessor = BasePreprocessor()\n",
    "#     streamers = flowersfactory.create_datastreamer(\n",
    "#         batchsize=batchsize, preprocessor=preprocessor\n",
    "#     )\n",
    "#     train = streamers[\"train\"]\n",
    "#     valid = streamers[\"valid\"]\n",
    "#     trainstreamer = train.stream()\n",
    "#     validstreamer = valid.stream()\n",
    "#     output_size = 5\n",
    "#     return trainstreamer, validstreamer, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34305788",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainstreamer, validstreamer, output_size = get_streamers(batch_size)\n",
    "\n",
    "x, y = next(trainstreamer)\n",
    "\n",
    "logger.info(f\"Fashion images shape: {x.shape}, labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad14e2",
   "metadata": {},
   "source": [
    "Set machine type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_device() -> str:\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc7b18",
   "metadata": {},
   "source": [
    "Setup MLFlow & work directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefab53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_mlflow() -> None:\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "\n",
    "setup_mlflow()\n",
    "\n",
    "def set_model_dir(model_dir: str) -> None:\n",
    "    modeldir = Path(model_dir).resolve()\n",
    "    if not modeldir.exists():\n",
    "        modeldir.mkdir(parents=True)\n",
    "        logger.info(f\"Created {modeldir}\")\n",
    "    return modeldir\n",
    "\n",
    "model_dir = set_model_dir(model_dir=\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f4f46",
   "metadata": {},
   "source": [
    "Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1708b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters,\n",
    "        units1=128,\n",
    "        units2=64,\n",
    "        output_size=10,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        pooling_layer=nn.MaxPool2d,\n",
    "        padding=\"valid\",\n",
    "        input_size=(batch_size, 1, 28, 28),\n",
    "        dropout1=0.5,\n",
    "        dropout2=0.3,\n",
    "        use_batchnorm=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = input_size[1]\n",
    "        self.input_size = input_size\n",
    "        self.filters = filters\n",
    "        self.units1 = units1\n",
    "        self.units2 = units2\n",
    "\n",
    "        pad = self._get_pad(padding, kernel_size)\n",
    "        layers = []\n",
    "        # First conv block\n",
    "        layers.append(nn.Conv2d(self.in_channels, filters, kernel_size=kernel_size, stride=stride, padding=pad))\n",
    "        if use_batchnorm:\n",
    "            layers.append(nn.BatchNorm2d(filters))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(pooling_layer(kernel_size=2))\n",
    "        # Second conv block\n",
    "        layers.append(nn.Conv2d(filters, filters, kernel_size=kernel_size, stride=stride, padding=pad))\n",
    "        if use_batchnorm:\n",
    "            layers.append(nn.BatchNorm2d(filters))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(pooling_layer(kernel_size=2))\n",
    "        # Third conv block\n",
    "        layers.append(nn.Conv2d(filters, filters, kernel_size=kernel_size, stride=stride, padding=pad))\n",
    "        if use_batchnorm:\n",
    "            layers.append(nn.BatchNorm2d(filters))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(pooling_layer(kernel_size=2))\n",
    "        self.convolutions = nn.Sequential(*layers)\n",
    "\n",
    "        activation_map_size = self._conv_test(input_size)\n",
    "        logger.info(f\"Aggregating activation map with size {activation_map_size}\")\n",
    "        self.agg = nn.AvgPool2d(activation_map_size)\n",
    "\n",
    "        dense_layers = []\n",
    "        dense_layers.append(nn.Flatten()),\n",
    "\n",
    "        dense_layers.append(nn.Dropout(p=dropout1)),\n",
    "        dense_layers.append(nn.Linear(filters, units1)),\n",
    "        if use_batchnorm:\n",
    "            dense_layers.append(nn.BatchNorm1d(units1))\n",
    "        dense_layers.append(nn.ReLU())\n",
    "        dense_layers.append(nn.Dropout(p=dropout2))\n",
    "        dense_layers.append(nn.Linear(units1, units2))\n",
    "        if use_batchnorm:\n",
    "            dense_layers.append(nn.BatchNorm1d(units2))\n",
    "        dense_layers.append(nn.ReLU())\n",
    "        dense_layers.append(nn.Linear(units2, output_size))\n",
    "        self.dense = nn.Sequential(*dense_layers)\n",
    "\n",
    "    def _get_pad(self, padding, kernel_size):\n",
    "        if isinstance(padding, str):\n",
    "            if padding == \"same\":\n",
    "                return (kernel_size - 1) // 2\n",
    "            elif padding == \"valid\":\n",
    "                return 0\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown padding: {padding}\")\n",
    "        return padding  # If already int\n",
    "\n",
    "    def _conv_test(self, input_size=(batch_size, 1, 28, 28)):\n",
    "        x = torch.ones(input_size)\n",
    "        x = self.convolutions(x)\n",
    "        return x.shape[-2:]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "        x = self.agg(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e7aff",
   "metadata": {},
   "source": [
    "Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65379702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, STATUS_OK, tpe, Trials\n",
    "from datetime import datetime\n",
    "from torch import optim, nn\n",
    "from hyperopt import hp\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    # End any previous MLflow run if still active\n",
    "    if mlflow.active_run() is not None:\n",
    "        mlflow.end_run()\n",
    "    try:\n",
    "        with mlflow.start_run():\n",
    "            mlflow.set_experiment(experiment_name)\n",
    "            mlflow.set_tag(\"model\", \"CNN\")\n",
    "            # Log parameters specific to this model\n",
    "            mlflow.log_params({\n",
    "                \"batch_size\": batch_size,\n",
    "                \"epochs\": params[\"epochs\"],\n",
    "                \"filters\": params[\"filters\"],\n",
    "                \"padding\": params[\"padding\"],\n",
    "                \"kernel_size\": params[\"kernel_size\"],\n",
    "                \"stride\": params[\"stride\"],\n",
    "                \"pooling_method\": params[\"pooling_method\"],\n",
    "                \"units1\": params[\"units1\"],\n",
    "                \"units2\": params[\"units2\"],\n",
    "                \"dropout1\": params[\"dropout1\"],\n",
    "                \"dropout2\": params[\"dropout2\"],\n",
    "                \"use_batchnorm\": params[\"use_batchnorm\"],\n",
    "            })\n",
    "            \n",
    "            filters = params[\"filters\"]\n",
    "            kernel_size = params[\"kernel_size\"]\n",
    "            stride = params[\"stride\"]\n",
    "            pooling_layer = params[\"pooling_method\"]\n",
    "            units1 = params[\"units1\"]\n",
    "            units2 = params[\"units2\"]\n",
    "            epochs = params[\"epochs\"]\n",
    "            padding = params[\"padding\"]\n",
    "            dropout1 = params.get(\"dropout1\", 0.5)\n",
    "            dropout2 = params.get(\"dropout2\", 0.3)\n",
    "\n",
    "            model = CustomCNN(\n",
    "                # input_size=(batch_size, 1, 28, 28),\n",
    "                input_size=x.shape,\n",
    "                filters=filters,\n",
    "                units1=units1,\n",
    "                units2=units2,\n",
    "                output_size=output_size,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                pooling_layer=pooling_layer,\n",
    "                padding=padding,\n",
    "                dropout1=dropout1,\n",
    "                dropout2=dropout2,\n",
    "                use_batchnorm=True,\n",
    "            ).to(device)\n",
    "\n",
    "            # Print model summary\n",
    "            logger.info(model)\n",
    "            logger.info(f\"Input size: {x.shape}\")\n",
    "            with torch.no_grad():\n",
    "                x_input = x.to(device)\n",
    "                if x_input.dim() == 2:\n",
    "                    x_input = x_input.unsqueeze(1)\n",
    "                for i, layer in enumerate(model.convolutions):\n",
    "                    x_input = layer(x_input)\n",
    "                logger.info(f\"After convolutions[{i}] ({layer.__class__.__name__}): {x_input.shape}\")\n",
    "                x_input = model.agg(x_input)\n",
    "                logger.info(f\"After agg ({model.agg.__class__.__name__}): {x_input.shape}\")\n",
    "                x_input = model.dense[0](x_input)\n",
    "                logger.info(f\"After dense[0] (Flatten): {x_input.shape}\")\n",
    "                x_input = model.dense[1](x_input)\n",
    "                logger.info(f\"After dense[1] (Linear): {x_input.shape}\")\n",
    "                x_input = model.dense[2](x_input)\n",
    "                logger.info(f\"After dense[2] (ReLU): {x_input.shape}\")\n",
    "                x_input = model.dense[3](x_input)\n",
    "                logger.info(f\"After dense[3] (Linear): {x_input.shape}\")\n",
    "                x_input = model.dense[4](x_input)\n",
    "                logger.info(f\"After dense[4] (ReLU): {x_input.shape}\")\n",
    "                x_input = model.dense[5](x_input)\n",
    "                logger.info(f\"After dense[5] (Linear): {x_input.shape}\")\n",
    "\n",
    "            train_settings = TrainerSettings(\n",
    "                epochs=epochs,\n",
    "                reporttypes=[ReportTypes.MLFLOW, ReportTypes.TOML],\n",
    "                metrics=[metrics.Accuracy()],\n",
    "                logdir=model_dir,\n",
    "                train_steps=100,\n",
    "                valid_steps=100,\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                optimizer=optim.Adam,\n",
    "                loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "                scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                traindataloader=trainstreamer,\n",
    "                validdataloader=validstreamer,\n",
    "                settings=train_settings,\n",
    "                device=device,\n",
    "            )\n",
    "            trainer.loop()\n",
    "\n",
    "            tag = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "            modelpath = model_dir / (tag + \"model.pt\")\n",
    "            logger.info(f\"Saving model to {modelpath}\")\n",
    "            torch.save(model, modelpath)\n",
    "\n",
    "            mlflow.log_artifact(local_path=str(modelpath), artifact_path=\"pytorch_models\")\n",
    "            return {\"loss\": trainer.test_loss, \"status\": STATUS_OK}\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Training failed due to error: {e}\")\n",
    "        return {\"loss\": 9999, \"status\": STATUS_OK}\n",
    "\n",
    "# search_space = {\n",
    "#     \"filters\": hp.choice(\"filters\", [8, 64, 128]),\n",
    "#     \"kernel_size\": hp.choice(\"kernel_size\", [1, 2, 3]),\n",
    "#     \"stride\": hp.choice(\"stride\", [1, 2]),\n",
    "#     \"pooling_method\": hp.choice(\"pooling_method\", [nn.MaxPool2d, nn.AvgPool2d]),\n",
    "#     \"padding\": hp.choice(\"padding\", [\"same\", \"valid\"]),\n",
    "#     \"units1\": hp.choice(\"units1\", [128]),\n",
    "#     \"units2\": hp.choice(\"units2\", [64]),\n",
    "#     \"epochs\": hp.choice(\"epochs\", [10]),\n",
    "# }\n",
    "\n",
    "# search_space = {\n",
    "#     \"filters\": hp.choice(\"filters\", [64, 128, 256]),\n",
    "#     \"kernel_size\": hp.choice(\"kernel_size\", [1, 2, 3]),\n",
    "#     \"stride\": hp.choice(\"stride\", [1, 2, 3]),\n",
    "#     \"pooling_method\": hp.choice(\"pooling_method\", [nn.MaxPool2d, nn.AvgPool2d]),\n",
    "#     # \"padding\": hp.choice(\"padding\", [\"same\", \"valid\"]),\n",
    "#     \"padding\": hp.choice(\"padding\", [\"valid\"]),\n",
    "#     \"units1\": hp.choice(\"units1\", [128]),\n",
    "#     \"units2\": hp.choice(\"units2\", [64]),\n",
    "#     \"epochs\": hp.choice(\"epochs\", [5]),\n",
    "# }\n",
    "\n",
    "search_space = {\n",
    "    \"filters\": hp.choice(\"filters\", [175]),\n",
    "    \"kernel_size\": hp.choice(\"kernel_size\", [2]),\n",
    "    \"stride\": hp.choice(\"stride\", [1]),\n",
    "    \"pooling_method\": hp.choice(\"pooling_method\", [nn.MaxPool2d]),\n",
    "    \"padding\": hp.choice(\"padding\", [\"same\"]),\n",
    "    \"units1\": hp.choice(\"units1\", [128]),\n",
    "    \"units2\": hp.choice(\"units2\", [64]),\n",
    "    \"epochs\": hp.choice(\"epochs\", [50]),\n",
    "    # \"dropout1\": hp.choice(\"dropout1\", [0, 0.3, 0.5]),\n",
    "    \"dropout1\": hp.choice(\"dropout1\", [0.6]),\n",
    "    # \"dropout2\": hp.choice(\"dropout2\", [0, 0.3, 0.5]),\n",
    "    \"dropout2\": hp.choice(\"dropout2\", [0.6]),\n",
    "    \"use_batchnorm\": hp.choice(\"use_batchnorm\", [True]),\n",
    "}\n",
    "\n",
    "\n",
    "experiment_name = \"Experiment gridsearch CNN improvements\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    max_evals=1,\n",
    "    algo=tpe.suggest,\n",
    "    trials=Trials()\n",
    ")\n",
    "\n",
    "logger.info(f\"Best result: {best_result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio-example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
